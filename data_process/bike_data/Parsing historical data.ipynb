{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_historical = glob.glob(\"*-cabi-trip-history-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-Q4-cabi-trip-history-data.csv\n",
      "2011-Q1-cabi-trip-history-data.csv\n",
      "2011-Q2-cabi-trip-history-data.csv\n",
      "2011-Q3-cabi-trip-history-data.csv\n",
      "2011-Q4-cabi-trip-history-data.csv\n",
      "2012-Q1-cabi-trip-history-data.csv\n",
      "2012-Q2-cabi-trip-history-data.csv\n",
      "2012-Q3-cabi-trip-history-data.csv\n",
      "2012-Q4-cabi-trip-history-data.csv\n",
      "2013-Q1-cabi-trip-history-data.csv\n",
      "2013-Q2-cabi-trip-history-data.csv\n",
      "2013-Q3-cabi-trip-history-data.csv\n",
      "2013-Q4-cabi-trip-history-data.csv\n",
      "2014-Q1-cabi-trip-history-data.csv\n",
      "2014-Q2-cabi-trip-history-data.csv\n",
      "2014-Q3-cabi-trip-history-data.csv\n",
      "2014-Q4-cabi-trip-history-data.csv\n",
      "2015-Q1-cabi-trip-history-data.csv\n",
      "2015-Q2-cabi-trip-history-data.csv\n",
      "2015-Q3-cabi-trip-history-data.csv\n",
      "2015-Q4-cabi-trip-history-data.csv\n",
      "2016-Q1-cabi-trip-history-data.csv\n",
      "2016-Q2-cabi-trip-history-data.csv\n"
     ]
    }
   ],
   "source": [
    "historical_df_list = []\n",
    "for each_file in all_historical:\n",
    "    print each_file\n",
    "    temp_df = pd.read_csv(each_file)\n",
    "    temp_df.columns = [each.lower() for each in temp_df.columns]\n",
    "    if 'start station number' in temp_df.columns:\n",
    "        # This is a updated version of dataset\n",
    "        temp_df = temp_df[['start date','end date','start station','start station number']]\n",
    "        historical_df_list.append(temp_df)\n",
    "    else:\n",
    "        # older ones \n",
    "        temp_df = temp_df[['start date','end date','start station']]\n",
    "        historical_df_list.append(temp_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "station_change = []\n",
    "i = 0\n",
    "for each_df in historical_df_list:\n",
    "    print i\n",
    "    # Filter out the changed names\n",
    "    extract_start = each_df['start station'].str.extract('(?P<extracted>[^\\[\\]]+) *(?P<change>\\[[^\\[\\]]+\\])? *', expand=False)\n",
    "#     extract_end = each_df['end station'].str.extract('(?P<extracted>[^\\[\\]]+) *(?P<change>\\[[^\\[\\]]+\\])? *', expand=False)\n",
    "    new_start = extract_start.extracted.apply(lambda x: str(x).rstrip())\n",
    "#     new_end = extract_end.extracted.apply(lambda x: str(x).rstrip())\n",
    "    each_df['start station'] = new_start\n",
    "#     each_df['end station'] = new_end\n",
    "    # Then get the station change\n",
    "    if extract_start.shape[1] > 1:\n",
    "        station_change.append(extract_start.dropna(how='any').drop_duplicates())\n",
    "#     if extract_end.shape[1] > 1:\n",
    "#         station_change.append(extract_end.dropna(how='any').drop_duplicates())\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'id', u'lat', u'long', u'name', u'terminalname'], dtype='object')\n",
      "lat             float64\n",
      "long            float64\n",
      "name             object\n",
      "terminalname      int64\n",
      "dtype: object\n",
      "start date              object\n",
      "end date                object\n",
      "start station           object\n",
      "start station number     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "station_df = pd.read_csv('station.csv')\n",
    "print station_df.columns\n",
    "useful_station_df = station_df[[u'lat', u'long', u'name', u'terminalname']]\n",
    "print useful_station_df.dtypes\n",
    "print historical_df_list[-1].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "combined = []\n",
    "i = 0\n",
    "# Find the useful information from the station_df\n",
    "for each_df in historical_df_list:\n",
    "    print i\n",
    "    if \"start station number\" in each_df.columns:\n",
    "        # merge on the numbers\n",
    "        merged = each_df.merge(useful_station_df, left_on='start station number', right_on='terminalname', how='inner')\n",
    "        combined.append(merged[['start date','end date','start station','terminalname', 'lat', 'long']])\n",
    "    else:\n",
    "        # have to merge on the names:\n",
    "        merged = each_df.merge(useful_station_df, left_on='start station', right_on='name', how='inner')\n",
    "        combined.append(merged[['start date','end date','start station','terminalname', 'lat', 'long']])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11815281, 6)\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.concat(combined)\n",
    "print total_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df.to_csv(\"merged_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df = total_df.sample(frac=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df.to_csv(\"sampled_merged_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
